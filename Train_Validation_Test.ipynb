{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "%matplotlib inline\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gender_submission.csv', 'test.csv', 'train.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('./data/test.csv')\n",
    "df_train = pd.read_csv('./data/train.csv')\n",
    "df_submission = pd.read_csv('./data/gender_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(df,option):\n",
    "    assert option in [\"test\",\"train\"] , \"Option must be test or train\"\n",
    "    df = df.set_index('PassengerId')\n",
    "    cond = df.Fare.isnull()\n",
    "    sub_value = df.Fare.mean()\n",
    "    df.Fare = np.where(cond, sub_value, df.Fare)\n",
    "    ticket_p = df.groupby('Ticket', as_index=False).agg({'Name': 'count'}).rename(columns={'Name':'t_count'}).sort_values('t_count', ascending=False)\n",
    "    df_merge = df.merge(ticket_p, on='Ticket')\n",
    "    df_merge['Fare_per_person'] = df_merge.Fare/df_merge.t_count\n",
    "    cond = df_merge.Age.isnull()\n",
    "    sub_value = df_merge.Age.mean()\n",
    "    df_merge.Age = np.where(cond, sub_value, df_merge.Age)\n",
    "    cond = (df_merge.Sex == 'female')\n",
    "    sub_value = 1\n",
    "    df_merge.Sex = np.where(cond, sub_value, 0)\n",
    "    cond = df_merge.Embarked.isnull()\n",
    "    sub_value = 'S'\n",
    "    df_merge.Embarked = np.where(cond, 'S', df_merge.Embarked)\n",
    "    df_merge = df_merge.drop(['Name', 'Ticket', 'Cabin'], axis=1)\n",
    "    df_merge.isnull().sum()\n",
    "    ohe = OneHotEncoder()\n",
    "    X_cat = ohe.fit_transform(df_merge.Embarked.values.reshape(-1,1)).toarray()\n",
    "    if option==\"train\":\n",
    "        X_short = df_merge.drop(['Survived','Embarked'], axis=1).values\n",
    "    else:\n",
    "        X_short = df_merge.drop(['Embarked'], axis=1).values\n",
    "    X = np.hstack([X_cat, X_short])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val=transform(df_train, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_val=df_train.Survived.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=transform(df_test,\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function train_test_split in module sklearn.model_selection._split:\n",
      "\n",
      "train_test_split(*arrays, **options)\n",
      "    Split arrays or matrices into random train and test subsets\n",
      "    \n",
      "    Quick utility that wraps input validation and\n",
      "    ``next(ShuffleSplit().split(X, y))`` and application to input data\n",
      "    into a single call for splitting (and optionally subsampling) data in a\n",
      "    oneliner.\n",
      "    \n",
      "    Read more in the :ref:`User Guide <cross_validation>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    *arrays : sequence of indexables with same length / shape[0]\n",
      "        Allowed inputs are lists, numpy arrays, scipy-sparse\n",
      "        matrices or pandas dataframes.\n",
      "    \n",
      "    test_size : float or int, default=None\n",
      "        If float, should be between 0.0 and 1.0 and represent the proportion\n",
      "        of the dataset to include in the test split. If int, represents the\n",
      "        absolute number of test samples. If None, the value is set to the\n",
      "        complement of the train size. If ``train_size`` is also None, it will\n",
      "        be set to 0.25.\n",
      "    \n",
      "    train_size : float or int, default=None\n",
      "        If float, should be between 0.0 and 1.0 and represent the\n",
      "        proportion of the dataset to include in the train split. If\n",
      "        int, represents the absolute number of train samples. If None,\n",
      "        the value is automatically set to the complement of the test size.\n",
      "    \n",
      "    random_state : int or RandomState instance, default=None\n",
      "        Controls the shuffling applied to the data before applying the split.\n",
      "        Pass an int for reproducible output across multiple function calls.\n",
      "        See :term:`Glossary <random_state>`.\n",
      "    \n",
      "    \n",
      "    shuffle : bool, default=True\n",
      "        Whether or not to shuffle the data before splitting. If shuffle=False\n",
      "        then stratify must be None.\n",
      "    \n",
      "    stratify : array-like, default=None\n",
      "        If not None, data is split in a stratified fashion, using this as\n",
      "        the class labels.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    splitting : list, length=2 * len(arrays)\n",
      "        List containing train-test split of inputs.\n",
      "    \n",
      "        .. versionadded:: 0.16\n",
      "            If the input is sparse, the output will be a\n",
      "            ``scipy.sparse.csr_matrix``. Else, output type is the same as the\n",
      "            input type.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> import numpy as np\n",
      "    >>> from sklearn.model_selection import train_test_split\n",
      "    >>> X, y = np.arange(10).reshape((5, 2)), range(5)\n",
      "    >>> X\n",
      "    array([[0, 1],\n",
      "           [2, 3],\n",
      "           [4, 5],\n",
      "           [6, 7],\n",
      "           [8, 9]])\n",
      "    >>> list(y)\n",
      "    [0, 1, 2, 3, 4]\n",
      "    \n",
      "    >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "    ...     X, y, test_size=0.33, random_state=42)\n",
      "    ...\n",
      "    >>> X_train\n",
      "    array([[4, 5],\n",
      "           [0, 1],\n",
      "           [6, 7]])\n",
      "    >>> y_train\n",
      "    [2, 0, 3]\n",
      "    >>> X_test\n",
      "    array([[2, 3],\n",
      "           [8, 9]])\n",
      "    >>> y_test\n",
      "    [1, 4]\n",
      "    \n",
      "    >>> train_test_split(y, shuffle=False)\n",
      "    [[0, 1, 2], [3, 4]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(train_test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((712, 11), (179, 11))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gergo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5865921787709497"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr=LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_val=lr.predict(X_val)\n",
    "acc_val=accuracy_score(y_val,y_pred_val)\n",
    "acc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LogisticRegression in module sklearn.linear_model._logistic:\n",
      "\n",
      "class LogisticRegression(sklearn.base.BaseEstimator, sklearn.linear_model._base.LinearClassifierMixin, sklearn.linear_model._base.SparseCoefMixin)\n",
      " |  LogisticRegression(penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
      " |  \n",
      " |  Logistic Regression (aka logit, MaxEnt) classifier.\n",
      " |  \n",
      " |  In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\n",
      " |  scheme if the 'multi_class' option is set to 'ovr', and uses the\n",
      " |  cross-entropy loss if the 'multi_class' option is set to 'multinomial'.\n",
      " |  (Currently the 'multinomial' option is supported only by the 'lbfgs',\n",
      " |  'sag', 'saga' and 'newton-cg' solvers.)\n",
      " |  \n",
      " |  This class implements regularized logistic regression using the\n",
      " |  'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note\n",
      " |  that regularization is applied by default**. It can handle both dense\n",
      " |  and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\n",
      " |  floats for optimal performance; any other input format will be converted\n",
      " |  (and copied).\n",
      " |  \n",
      " |  The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n",
      " |  with primal formulation, or no regularization. The 'liblinear' solver\n",
      " |  supports both L1 and L2 regularization, with a dual formulation only for\n",
      " |  the L2 penalty. The Elastic-Net regularization is only supported by the\n",
      " |  'saga' solver.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <logistic_regression>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  penalty : {'l1', 'l2', 'elasticnet', 'none'}, default='l2'\n",
      " |      Used to specify the norm used in the penalization. The 'newton-cg',\n",
      " |      'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is\n",
      " |      only supported by the 'saga' solver. If 'none' (not supported by the\n",
      " |      liblinear solver), no regularization is applied.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |         l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n",
      " |  \n",
      " |  dual : bool, default=False\n",
      " |      Dual or primal formulation. Dual formulation is only implemented for\n",
      " |      l2 penalty with liblinear solver. Prefer dual=False when\n",
      " |      n_samples > n_features.\n",
      " |  \n",
      " |  tol : float, default=1e-4\n",
      " |      Tolerance for stopping criteria.\n",
      " |  \n",
      " |  C : float, default=1.0\n",
      " |      Inverse of regularization strength; must be a positive float.\n",
      " |      Like in support vector machines, smaller values specify stronger\n",
      " |      regularization.\n",
      " |  \n",
      " |  fit_intercept : bool, default=True\n",
      " |      Specifies if a constant (a.k.a. bias or intercept) should be\n",
      " |      added to the decision function.\n",
      " |  \n",
      " |  intercept_scaling : float, default=1\n",
      " |      Useful only when the solver 'liblinear' is used\n",
      " |      and self.fit_intercept is set to True. In this case, x becomes\n",
      " |      [x, self.intercept_scaling],\n",
      " |      i.e. a \"synthetic\" feature with constant value equal to\n",
      " |      intercept_scaling is appended to the instance vector.\n",
      " |      The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
      " |  \n",
      " |      Note! the synthetic feature weight is subject to l1/l2 regularization\n",
      " |      as all other features.\n",
      " |      To lessen the effect of regularization on synthetic feature weight\n",
      " |      (and therefore on the intercept) intercept_scaling has to be increased.\n",
      " |  \n",
      " |  class_weight : dict or 'balanced', default=None\n",
      " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      " |      If not given, all classes are supposed to have weight one.\n",
      " |  \n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
      " |  \n",
      " |      Note that these weights will be multiplied with sample_weight (passed\n",
      " |      through the fit method) if sample_weight is specified.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *class_weight='balanced'*\n",
      " |  \n",
      " |  random_state : int, RandomState instance, default=None\n",
      " |      Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n",
      " |      data. See :term:`Glossary <random_state>` for details.\n",
      " |  \n",
      " |  solver : {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},             default='lbfgs'\n",
      " |  \n",
      " |      Algorithm to use in the optimization problem.\n",
      " |  \n",
      " |      - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n",
      " |        'saga' are faster for large ones.\n",
      " |      - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n",
      " |        handle multinomial loss; 'liblinear' is limited to one-versus-rest\n",
      " |        schemes.\n",
      " |      - 'newton-cg', 'lbfgs', 'sag' and 'saga' handle L2 or no penalty\n",
      " |      - 'liblinear' and 'saga' also handle L1 penalty\n",
      " |      - 'saga' also supports 'elasticnet' penalty\n",
      " |      - 'liblinear' does not support setting ``penalty='none'``\n",
      " |  \n",
      " |      Note that 'sag' and 'saga' fast convergence is only guaranteed on\n",
      " |      features with approximately the same scale. You can\n",
      " |      preprocess the data with a scaler from sklearn.preprocessing.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         Stochastic Average Gradient descent solver.\n",
      " |      .. versionadded:: 0.19\n",
      " |         SAGA solver.\n",
      " |      .. versionchanged:: 0.22\n",
      " |          The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n",
      " |  \n",
      " |  max_iter : int, default=100\n",
      " |      Maximum number of iterations taken for the solvers to converge.\n",
      " |  \n",
      " |  multi_class : {'auto', 'ovr', 'multinomial'}, default='auto'\n",
      " |      If the option chosen is 'ovr', then a binary problem is fit for each\n",
      " |      label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
      " |      across the entire probability distribution, *even when the data is\n",
      " |      binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
      " |      'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
      " |      and otherwise selects 'multinomial'.\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |         Stochastic Average Gradient descent solver for 'multinomial' case.\n",
      " |      .. versionchanged:: 0.22\n",
      " |          Default changed from 'ovr' to 'auto' in 0.22.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      For the liblinear and lbfgs solvers set verbose to any positive\n",
      " |      number for verbosity.\n",
      " |  \n",
      " |  warm_start : bool, default=False\n",
      " |      When set to True, reuse the solution of the previous call to fit as\n",
      " |      initialization, otherwise, just erase the previous solution.\n",
      " |      Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      Number of CPU cores used when parallelizing over classes if\n",
      " |      multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n",
      " |      set to 'liblinear' regardless of whether 'multi_class' is specified or\n",
      " |      not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      " |      context. ``-1`` means using all processors.\n",
      " |      See :term:`Glossary <n_jobs>` for more details.\n",
      " |  \n",
      " |  l1_ratio : float, default=None\n",
      " |      The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n",
      " |      used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n",
      " |      to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n",
      " |      to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n",
      " |      combination of L1 and L2.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  \n",
      " |  classes_ : ndarray of shape (n_classes, )\n",
      " |      A list of class labels known to the classifier.\n",
      " |  \n",
      " |  coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
      " |      Coefficient of the features in the decision function.\n",
      " |  \n",
      " |      `coef_` is of shape (1, n_features) when the given problem is binary.\n",
      " |      In particular, when `multi_class='multinomial'`, `coef_` corresponds\n",
      " |      to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n",
      " |  \n",
      " |  intercept_ : ndarray of shape (1,) or (n_classes,)\n",
      " |      Intercept (a.k.a. bias) added to the decision function.\n",
      " |  \n",
      " |      If `fit_intercept` is set to False, the intercept is set to zero.\n",
      " |      `intercept_` is of shape (1,) when the given problem is binary.\n",
      " |      In particular, when `multi_class='multinomial'`, `intercept_`\n",
      " |      corresponds to outcome 1 (True) and `-intercept_` corresponds to\n",
      " |      outcome 0 (False).\n",
      " |  \n",
      " |  n_iter_ : ndarray of shape (n_classes,) or (1, )\n",
      " |      Actual number of iterations for all classes. If binary or multinomial,\n",
      " |      it returns only 1 element. For liblinear solver, only the maximum\n",
      " |      number of iteration across all classes is given.\n",
      " |  \n",
      " |      .. versionchanged:: 0.20\n",
      " |  \n",
      " |          In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n",
      " |          ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  SGDClassifier : Incrementally trained logistic regression (when given\n",
      " |      the parameter ``loss=\"log\"``).\n",
      " |  LogisticRegressionCV : Logistic regression with built-in cross validation.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The underlying C implementation uses a random number generator to\n",
      " |  select features when fitting the model. It is thus not uncommon,\n",
      " |  to have slightly different results for the same input data. If\n",
      " |  that happens, try with a smaller tol parameter.\n",
      " |  \n",
      " |  Predict output may not match that of standalone liblinear in certain\n",
      " |  cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
      " |  in the narrative documentation.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  \n",
      " |  L-BFGS-B -- Software for Large-scale Bound-constrained Optimization\n",
      " |      Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\n",
      " |      http://users.iems.northwestern.edu/~nocedal/lbfgsb.html\n",
      " |  \n",
      " |  LIBLINEAR -- A Library for Large Linear Classification\n",
      " |      https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n",
      " |  \n",
      " |  SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n",
      " |      Minimizing Finite Sums with the Stochastic Average Gradient\n",
      " |      https://hal.inria.fr/hal-00860051/document\n",
      " |  \n",
      " |  SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n",
      " |      SAGA: A Fast Incremental Gradient Method With Support\n",
      " |      for Non-Strongly Convex Composite Objectives\n",
      " |      https://arxiv.org/abs/1407.0202\n",
      " |  \n",
      " |  Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n",
      " |      methods for logistic regression and maximum entropy models.\n",
      " |      Machine Learning 85(1-2):41-75.\n",
      " |      https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.datasets import load_iris\n",
      " |  >>> from sklearn.linear_model import LogisticRegression\n",
      " |  >>> X, y = load_iris(return_X_y=True)\n",
      " |  >>> clf = LogisticRegression(random_state=0).fit(X, y)\n",
      " |  >>> clf.predict(X[:2, :])\n",
      " |  array([0, 0])\n",
      " |  >>> clf.predict_proba(X[:2, :])\n",
      " |  array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n",
      " |         [9.7...e-01, 2.8...e-02, ...e-08]])\n",
      " |  >>> clf.score(X, y)\n",
      " |  0.97...\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LogisticRegression\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.linear_model._base.LinearClassifierMixin\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      sklearn.linear_model._base.SparseCoefMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit the model according to the given training data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training vector, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target vector relative to X.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,) default=None\n",
      " |          Array of weights that are assigned to individual samples.\n",
      " |          If not provided, then each sample is given unit weight.\n",
      " |      \n",
      " |          .. versionadded:: 0.17\n",
      " |             *sample_weight* support to LogisticRegression.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The SAGA solver supports both float64 and float32 bit arrays.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict logarithm of probability estimates.\n",
      " |      \n",
      " |      The returned estimates for all classes are ordered by the\n",
      " |      label of classes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Vector to be scored, where `n_samples` is the number of samples and\n",
      " |          `n_features` is the number of features.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      T : array-like of shape (n_samples, n_classes)\n",
      " |          Returns the log-probability of the sample for each class in the\n",
      " |          model, where classes are ordered as they are in ``self.classes_``.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Probability estimates.\n",
      " |      \n",
      " |      The returned estimates for all classes are ordered by the\n",
      " |      label of classes.\n",
      " |      \n",
      " |      For a multi_class problem, if multi_class is set to be \"multinomial\"\n",
      " |      the softmax function is used to find the predicted probability of\n",
      " |      each class.\n",
      " |      Else use a one-vs-rest approach, i.e calculate the probability\n",
      " |      of each class assuming it to be positive using the logistic function.\n",
      " |      and normalize these values across all the classes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Vector to be scored, where `n_samples` is the number of samples and\n",
      " |          `n_features` is the number of features.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      T : array-like of shape (n_samples, n_classes)\n",
      " |          Returns the probability of the sample for each class in the model,\n",
      " |          where classes are ordered as they are in ``self.classes_``.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Predict confidence scores for samples.\n",
      " |      \n",
      " |      The confidence score for a sample is the signed distance of that\n",
      " |      sample to the hyperplane.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n",
      " |          Confidence scores per (sample, class) combination. In the binary\n",
      " |          case, confidence score for self.classes_[1] where >0 means this\n",
      " |          class would be predicted.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class labels for samples in X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array, shape [n_samples]\n",
      " |          Predicted class label per sample.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of self.predict(X) wrt. y.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      " |  \n",
      " |  densify(self)\n",
      " |      Convert coefficient matrix to dense array format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      " |      default format of ``coef_`` and is required for fitting, so calling\n",
      " |      this method is only required on models that have previously been\n",
      " |      sparsified; otherwise, it is a no-op.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  sparsify(self)\n",
      " |      Convert coefficient matrix to sparse format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      " |      L1-regularized models can be much more memory- and storage-efficient\n",
      " |      than the usual numpy.ndarray representation.\n",
      " |      \n",
      " |      The ``intercept_`` member is not converted.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      " |      this may actually *increase* memory usage, so use this method with\n",
      " |      care. A rule of thumb is that the number of zero elements, which can\n",
      " |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      " |      to provide significant benefits.\n",
      " |      \n",
      " |      After calling this method, further fitting with the partial_fit\n",
      " |      method (if any) will not work until you call densify.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(LogisticRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000000e-04, 4.21696503e-03, 1.77827941e-01, 7.49894209e+00,\n",
       "       3.16227766e+02, 1.33352143e+04, 5.62341325e+05, 2.37137371e+07,\n",
       "       1.00000000e+09])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C_grid=np.logspace(-4,9,9)\n",
    "C_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1db9b4e2850>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc_vals=[]\n",
    "acc_trains=[]\n",
    "for c in C_grid:\n",
    "    lr1=LogisticRegression(C=c,max_iter=500)\n",
    "    lr1.fit(X_train, y_train)\n",
    "    y_pred_val=lr1.predict(X_val)\n",
    "    y_pred_train=lr1.predict(X_train)\n",
    "    acc_train=accuracy_score(y_train,y_pred_train)\n",
    "    acc_val=accuracy_score(y_val,y_pred_val)\n",
    "    acc_vals.append(acc_val)\n",
    "    acc_trains.append(acc_train)\n",
    "plt.scatter(C_grid,acc_vals)\n",
    "plt.xscale('log')\n",
    "plt.scatter(C_grid,acc_trains)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
