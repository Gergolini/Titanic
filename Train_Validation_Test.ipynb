{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "%matplotlib inline\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from utils import update_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test.csv', 'train.csv', 'gender_submission.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('./data/test.csv')\n",
    "df_train = pd.read_csv('./data/train.csv')\n",
    "df_submission = pd.read_csv('./data/gender_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_val = df_train.Survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(df,option):\n",
    "    assert option in [\"test\",\"train\"] , \"Option must be test or train\"\n",
    "    df = df.set_index('PassengerId')\n",
    "    cond = df.Fare.isnull()\n",
    "    sub_value = df.Fare.mean()\n",
    "    df.Fare = np.where(cond, sub_value, df.Fare)\n",
    "    ticket_p = df.groupby('Ticket', as_index=False).agg({'Name': 'count'}).rename(columns={'Name':'t_count'}).sort_values('t_count', ascending=False)\n",
    "    d = dict()\n",
    "    for i, row in ticket_p.iterrows():\n",
    "        d[row.Ticket] = row.t_count\n",
    "    df_merge = df.copy()\n",
    "    df_merge['t_count'] = df_merge.Ticket.apply(lambda x: d[x])\n",
    "    df_merge['Fare_per_person'] = df_merge.Fare/df_merge.t_count\n",
    "    cond = df_merge.Age.isnull()\n",
    "    sub_value = df_merge.Age.mean()\n",
    "    df_merge.Age = np.where(cond, sub_value, df_merge.Age)\n",
    "    cond = (df_merge.Sex == 'female')\n",
    "    sub_value = 1\n",
    "    df_merge.Sex = np.where(cond, sub_value, 0)\n",
    "    cond = df_merge.Embarked.isnull()\n",
    "    sub_value = 'S'\n",
    "    df_merge.Embarked = np.where(cond, 'S', df_merge.Embarked)\n",
    "    df_merge = df_merge.drop(['Name', 'Ticket', 'Cabin'], axis=1)\n",
    "    df_merge.isnull().sum()\n",
    "    ohe = OneHotEncoder()\n",
    "    X_cat = ohe.fit_transform(df_merge.Embarked.values.reshape(-1,1)).toarray()\n",
    "    if option==\"train\":\n",
    "        X_short = df_merge.drop(['Survived','Embarked'], axis=1).values\n",
    "    else:\n",
    "        X_short = df_merge.drop(['Embarked'], axis=1).values\n",
    "    X = np.hstack([X_cat, X_short])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val=transform(df_train, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((712, 11), (179, 11))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8156424581005587"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr=LogisticRegression(max_iter=500)\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_val=lr.predict(X_val)\n",
    "acc_val=accuracy_score(y_val,y_pred_val)\n",
    "acc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000000e-04, 4.21696503e-03, 1.77827941e-01, 7.49894209e+00,\n",
       "       3.16227766e+02, 1.33352143e+04, 5.62341325e+05, 2.37137371e+07,\n",
       "       1.00000000e+09])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C_grid=np.logspace(-4,9,9)\n",
    "C_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x12fa65668>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc_vals=[]\n",
    "acc_trains=[]\n",
    "for c in C_grid:\n",
    "    lr1=LogisticRegression(C=c,max_iter=500)\n",
    "    lr1.fit(X_train, y_train)\n",
    "    y_pred_val=lr1.predict(X_val)\n",
    "    y_pred_train=lr1.predict(X_train)\n",
    "    acc_train=accuracy_score(y_train,y_pred_train)\n",
    "    acc_val=accuracy_score(y_val,y_pred_val)\n",
    "    acc_vals.append(acc_val)\n",
    "    acc_trains.append(acc_train)\n",
    "\n",
    "plt.plot(C_grid,acc_vals,'-o', label='validation')\n",
    "plt.xscale('log')\n",
    "plt.plot(C_grid,acc_trains,'-o', label='training')\n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6815642458100558,\n",
       " 0.7262569832402235,\n",
       " 0.8100558659217877,\n",
       " 0.8156424581005587,\n",
       " 0.8156424581005587,\n",
       " 0.8156424581005587,\n",
       " 0.8156424581005587,\n",
       " 0.8156424581005587,\n",
       " 0.8156424581005587]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.argmax(acc_vals)\n",
    "C_best=C_grid[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(418,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr1=LogisticRegression(C=C_best,max_iter=500)\n",
    "lr1.fit(X_train, y_train)\n",
    "y_pred_test=lr1.predict(X_test)\n",
    "y_pred_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission['Survived']=y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Each sample has to be used for validation once and train model for each fold and check accuracy'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"HW:Write a code that will manually split the X_train_val and y_train_val into five folds\"\n",
    "\"Fold: in each fold 80% training and 20% validation\"\n",
    "\"Each sample has to be used for validation once and train model for each fold and check accuracy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_manual(X,y,num_folds):\n",
    "    n_perfold=int(len(X)/num_folds)\n",
    "    acc_vals=[]\n",
    "    for i in range(num_folds):\n",
    "        X_val_fold=X[n_perfold*i:n_perfold+n_perfold*i]\n",
    "        y_val_fold=y[n_perfold*i:n_perfold+n_perfold*i]\n",
    "        X_train_fold=np.concatenate((X[0:n_perfold*i],X[n_perfold+n_perfold*i:]),axis=0)\n",
    "        y_train_fold=np.concatenate((y[0:n_perfold*i],y[n_perfold+n_perfold*i:]),axis=0)\n",
    "        print(\"{} {} {} {}\".format(X_val_fold.shape, y_val_fold.shape,X_train_fold.shape,  y_train_fold.shape ))\n",
    "        lr1=LogisticRegression(C=C_best,max_iter=500)\n",
    "        lr1.fit(X_train_fold, y_train_fold)\n",
    "        y_pred_val=lr1.predict(X_val_fold)\n",
    "        acc_val=accuracy_score(y_val_fold,y_pred_val)\n",
    "        acc_vals.append(acc_val)\n",
    "    return (acc_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(712, 11)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(142, 11) (142,) (570, 11) (570,)\n",
      "(142, 11) (142,) (570, 11) (570,)\n",
      "(142, 11) (142,) (570, 11) (570,)\n",
      "(142, 11) (142,) (570, 11) (570,)\n",
      "(142, 11) (142,) (570, 11) (570,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8169014084507042,\n",
       " 0.8098591549295775,\n",
       " 0.8098591549295775,\n",
       " 0.7535211267605634,\n",
       " 0.8169014084507042]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_manual(X_train,y_train,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HW: Read about how sklearn does cross validation and how the function works and implement it to get the list with one line of code'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"HW: Read about how sklearn does cross validation and how the function works and implement it to get the list with one line of code\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lawliet/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.78212291, 0.78089888, 0.78089888, 0.76966292, 0.8258427 ])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr1=LogisticRegression(C=C_best,max_iter=500)\n",
    "cross_val_score(lr1, X_train_val,y_train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HW2: Come up with a way that undivisible samples can be split into uneven folds and still only use the samples only once for validation'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"HW2: Come up with a way that undivisible samples can be split into uneven folds and still only use the samples only once for validation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(143, 11) (143,) (570, 11) (570,)\n",
      "(143, 11) (143,) (570, 11) (570,)\n",
      "(143, 11) (143,) (570, 11) (570,)\n",
      "(143, 11) (143,) (570, 11) (570,)\n",
      "(140, 11) (140,) (572, 11) (572,)\n",
      "[0.8111888111888111, 0.8251748251748252, 0.8041958041958042, 0.7482517482517482, 0.8]\n"
     ]
    }
   ],
   "source": [
    "def split_manual(X,y,num_folds):\n",
    "    n_perfold=int(len(X)/num_folds)\n",
    "    remain=len(X)%num_folds\n",
    "    acc_vals=[]\n",
    "    g=0\n",
    "    for i in range(num_folds):\n",
    "        X_val_fold=X[n_perfold*i+g:n_perfold+n_perfold*i+1+g]\n",
    "        y_val_fold=y[n_perfold*i+g:n_perfold+n_perfold*i+1+g]\n",
    "        X_train_fold=np.concatenate((X[0:n_perfold*i+g],X[n_perfold+g+n_perfold*i:]),axis=0)\n",
    "        y_train_fold=np.concatenate((y[0:n_perfold*i+g],y[n_perfold+g+n_perfold*i:]),axis=0)\n",
    "        g+=1\n",
    "        print(\"{} {} {} {}\".format(X_val_fold.shape, y_val_fold.shape,X_train_fold.shape,  y_train_fold.shape ))\n",
    "        lr1=LogisticRegression(C=C_best,max_iter=500)\n",
    "        lr1.fit(X_train_fold, y_train_fold)\n",
    "        y_pred_val=lr1.predict(X_val_fold)\n",
    "        acc_val=accuracy_score(y_val_fold,y_pred_val)\n",
    "        acc_vals.append(acc_val)\n",
    "    return (acc_vals)\n",
    "print(split_manual(X_train,y_train,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ex: 11 samples and 3 folds'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Ex: 11 samples and 3 folds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ex: 109 samples and 10 folds'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Ex: 109 samples and 10 folds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.812623187496077"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt=DecisionTreeClassifier(max_depth=10,min_samples_split=20)\n",
    "np.mean(cross_val_score(dt, X_train_val,y_train_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 21, 0.8159876969430669)\n"
     ]
    }
   ],
   "source": [
    "def DTC(a,b):\n",
    "    best_depth=0\n",
    "    best_split=0\n",
    "    best_cv=0\n",
    "    for i in range(5,15,1):\n",
    "        for g in range(15,25,1):\n",
    "            dt=DecisionTreeClassifier(max_depth=i,min_samples_split=g)\n",
    "            cv_score=cross_val_score(dt, X_train_val,y_train_val).mean()\n",
    "            #print(i,g,cv_score)\n",
    "            if(cv_score>best_cv):\n",
    "                best_cv=cv_score\n",
    "                best_depth=i\n",
    "                best_split=g\n",
    "    return best_depth, best_split, best_cv\n",
    "print(DTC(30,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class GridSearchCV in module sklearn.model_selection._search:\n",
      "\n",
      "class GridSearchCV(BaseSearchCV)\n",
      " |  GridSearchCV(estimator, param_grid, *, scoring=None, n_jobs=None, iid='deprecated', refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs', error_score=nan, return_train_score=False)\n",
      " |  \n",
      " |  Exhaustive search over specified parameter values for an estimator.\n",
      " |  \n",
      " |  Important members are fit, predict.\n",
      " |  \n",
      " |  GridSearchCV implements a \"fit\" and a \"score\" method.\n",
      " |  It also implements \"predict\", \"predict_proba\", \"decision_function\",\n",
      " |  \"transform\" and \"inverse_transform\" if they are implemented in the\n",
      " |  estimator used.\n",
      " |  \n",
      " |  The parameters of the estimator used to apply these methods are optimized\n",
      " |  by cross-validated grid-search over a parameter grid.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <grid_search>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  estimator : estimator object.\n",
      " |      This is assumed to implement the scikit-learn estimator interface.\n",
      " |      Either estimator needs to provide a ``score`` function,\n",
      " |      or ``scoring`` must be passed.\n",
      " |  \n",
      " |  param_grid : dict or list of dictionaries\n",
      " |      Dictionary with parameters names (`str`) as keys and lists of\n",
      " |      parameter settings to try as values, or a list of such\n",
      " |      dictionaries, in which case the grids spanned by each dictionary\n",
      " |      in the list are explored. This enables searching over any sequence\n",
      " |      of parameter settings.\n",
      " |  \n",
      " |  scoring : str, callable, list/tuple or dict, default=None\n",
      " |      A single str (see :ref:`scoring_parameter`) or a callable\n",
      " |      (see :ref:`scoring`) to evaluate the predictions on the test set.\n",
      " |  \n",
      " |      For evaluating multiple metrics, either give a list of (unique) strings\n",
      " |      or a dict with names as keys and callables as values.\n",
      " |  \n",
      " |      NOTE that when using custom scorers, each scorer should return a single\n",
      " |      value. Metric functions returning a list/array of values can be wrapped\n",
      " |      into multiple scorers that return one value each.\n",
      " |  \n",
      " |      See :ref:`multimetric_grid_search` for an example.\n",
      " |  \n",
      " |      If None, the estimator's score method is used.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      Number of jobs to run in parallel.\n",
      " |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      " |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      " |      for more details.\n",
      " |  \n",
      " |      .. versionchanged:: v0.20\n",
      " |         `n_jobs` default changed from 1 to None\n",
      " |  \n",
      " |  pre_dispatch : int, or str, default=n_jobs\n",
      " |      Controls the number of jobs that get dispatched during parallel\n",
      " |      execution. Reducing this number can be useful to avoid an\n",
      " |      explosion of memory consumption when more jobs get dispatched\n",
      " |      than CPUs can process. This parameter can be:\n",
      " |  \n",
      " |          - None, in which case all the jobs are immediately\n",
      " |            created and spawned. Use this for lightweight and\n",
      " |            fast-running jobs, to avoid delays due to on-demand\n",
      " |            spawning of the jobs\n",
      " |  \n",
      " |          - An int, giving the exact number of total jobs that are\n",
      " |            spawned\n",
      " |  \n",
      " |          - A str, giving an expression as a function of n_jobs,\n",
      " |            as in '2*n_jobs'\n",
      " |  \n",
      " |  iid : bool, default=False\n",
      " |      If True, return the average score across folds, weighted by the number\n",
      " |      of samples in each test set. In this case, the data is assumed to be\n",
      " |      identically distributed across the folds, and the loss minimized is\n",
      " |      the total loss per sample, and not the mean loss across the folds.\n",
      " |  \n",
      " |      .. deprecated:: 0.22\n",
      " |          Parameter ``iid`` is deprecated in 0.22 and will be removed in 0.24\n",
      " |  \n",
      " |  cv : int, cross-validation generator or an iterable, default=None\n",
      " |      Determines the cross-validation splitting strategy.\n",
      " |      Possible inputs for cv are:\n",
      " |  \n",
      " |      - None, to use the default 5-fold cross validation,\n",
      " |      - integer, to specify the number of folds in a `(Stratified)KFold`,\n",
      " |      - :term:`CV splitter`,\n",
      " |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      " |  \n",
      " |      For integer/None inputs, if the estimator is a classifier and ``y`` is\n",
      " |      either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
      " |      other cases, :class:`KFold` is used.\n",
      " |  \n",
      " |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      " |      cross-validation strategies that can be used here.\n",
      " |  \n",
      " |      .. versionchanged:: 0.22\n",
      " |          ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      " |  \n",
      " |  refit : bool, str, or callable, default=True\n",
      " |      Refit an estimator using the best found parameters on the whole\n",
      " |      dataset.\n",
      " |  \n",
      " |      For multiple metric evaluation, this needs to be a `str` denoting the\n",
      " |      scorer that would be used to find the best parameters for refitting\n",
      " |      the estimator at the end.\n",
      " |  \n",
      " |      Where there are considerations other than maximum score in\n",
      " |      choosing a best estimator, ``refit`` can be set to a function which\n",
      " |      returns the selected ``best_index_`` given ``cv_results_``. In that\n",
      " |      case, the ``best_estimator_`` and ``best_params_`` will be set\n",
      " |      according to the returned ``best_index_`` while the ``best_score_``\n",
      " |      attribute will not be available.\n",
      " |  \n",
      " |      The refitted estimator is made available at the ``best_estimator_``\n",
      " |      attribute and permits using ``predict`` directly on this\n",
      " |      ``GridSearchCV`` instance.\n",
      " |  \n",
      " |      Also for multiple metric evaluation, the attributes ``best_index_``,\n",
      " |      ``best_score_`` and ``best_params_`` will only be available if\n",
      " |      ``refit`` is set and all of them will be determined w.r.t this specific\n",
      " |      scorer.\n",
      " |  \n",
      " |      See ``scoring`` parameter to know more about multiple metric\n",
      " |      evaluation.\n",
      " |  \n",
      " |      .. versionchanged:: 0.20\n",
      " |          Support for callable added.\n",
      " |  \n",
      " |  verbose : integer\n",
      " |      Controls the verbosity: the higher, the more messages.\n",
      " |  \n",
      " |  error_score : 'raise' or numeric, default=np.nan\n",
      " |      Value to assign to the score if an error occurs in estimator fitting.\n",
      " |      If set to 'raise', the error is raised. If a numeric value is given,\n",
      " |      FitFailedWarning is raised. This parameter does not affect the refit\n",
      " |      step, which will always raise the error.\n",
      " |  \n",
      " |  return_train_score : bool, default=False\n",
      " |      If ``False``, the ``cv_results_`` attribute will not include training\n",
      " |      scores.\n",
      " |      Computing training scores is used to get insights on how different\n",
      " |      parameter settings impact the overfitting/underfitting trade-off.\n",
      " |      However computing the scores on the training set can be computationally\n",
      " |      expensive and is not strictly required to select the parameters that\n",
      " |      yield the best generalization performance.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |      .. versionchanged:: 0.21\n",
      " |          Default value was changed from ``True`` to ``False``\n",
      " |  \n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn import svm, datasets\n",
      " |  >>> from sklearn.model_selection import GridSearchCV\n",
      " |  >>> iris = datasets.load_iris()\n",
      " |  >>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
      " |  >>> svc = svm.SVC()\n",
      " |  >>> clf = GridSearchCV(svc, parameters)\n",
      " |  >>> clf.fit(iris.data, iris.target)\n",
      " |  GridSearchCV(estimator=SVC(),\n",
      " |               param_grid={'C': [1, 10], 'kernel': ('linear', 'rbf')})\n",
      " |  >>> sorted(clf.cv_results_.keys())\n",
      " |  ['mean_fit_time', 'mean_score_time', 'mean_test_score',...\n",
      " |   'param_C', 'param_kernel', 'params',...\n",
      " |   'rank_test_score', 'split0_test_score',...\n",
      " |   'split2_test_score', ...\n",
      " |   'std_fit_time', 'std_score_time', 'std_test_score']\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  cv_results_ : dict of numpy (masked) ndarrays\n",
      " |      A dict with keys as column headers and values as columns, that can be\n",
      " |      imported into a pandas ``DataFrame``.\n",
      " |  \n",
      " |      For instance the below given table\n",
      " |  \n",
      " |      +------------+-----------+------------+-----------------+---+---------+\n",
      " |      |param_kernel|param_gamma|param_degree|split0_test_score|...|rank_t...|\n",
      " |      +============+===========+============+=================+===+=========+\n",
      " |      |  'poly'    |     --    |      2     |       0.80      |...|    2    |\n",
      " |      +------------+-----------+------------+-----------------+---+---------+\n",
      " |      |  'poly'    |     --    |      3     |       0.70      |...|    4    |\n",
      " |      +------------+-----------+------------+-----------------+---+---------+\n",
      " |      |  'rbf'     |     0.1   |     --     |       0.80      |...|    3    |\n",
      " |      +------------+-----------+------------+-----------------+---+---------+\n",
      " |      |  'rbf'     |     0.2   |     --     |       0.93      |...|    1    |\n",
      " |      +------------+-----------+------------+-----------------+---+---------+\n",
      " |  \n",
      " |      will be represented by a ``cv_results_`` dict of::\n",
      " |  \n",
      " |          {\n",
      " |          'param_kernel': masked_array(data = ['poly', 'poly', 'rbf', 'rbf'],\n",
      " |                                       mask = [False False False False]...)\n",
      " |          'param_gamma': masked_array(data = [-- -- 0.1 0.2],\n",
      " |                                      mask = [ True  True False False]...),\n",
      " |          'param_degree': masked_array(data = [2.0 3.0 -- --],\n",
      " |                                       mask = [False False  True  True]...),\n",
      " |          'split0_test_score'  : [0.80, 0.70, 0.80, 0.93],\n",
      " |          'split1_test_score'  : [0.82, 0.50, 0.70, 0.78],\n",
      " |          'mean_test_score'    : [0.81, 0.60, 0.75, 0.85],\n",
      " |          'std_test_score'     : [0.01, 0.10, 0.05, 0.08],\n",
      " |          'rank_test_score'    : [2, 4, 3, 1],\n",
      " |          'split0_train_score' : [0.80, 0.92, 0.70, 0.93],\n",
      " |          'split1_train_score' : [0.82, 0.55, 0.70, 0.87],\n",
      " |          'mean_train_score'   : [0.81, 0.74, 0.70, 0.90],\n",
      " |          'std_train_score'    : [0.01, 0.19, 0.00, 0.03],\n",
      " |          'mean_fit_time'      : [0.73, 0.63, 0.43, 0.49],\n",
      " |          'std_fit_time'       : [0.01, 0.02, 0.01, 0.01],\n",
      " |          'mean_score_time'    : [0.01, 0.06, 0.04, 0.04],\n",
      " |          'std_score_time'     : [0.00, 0.00, 0.00, 0.01],\n",
      " |          'params'             : [{'kernel': 'poly', 'degree': 2}, ...],\n",
      " |          }\n",
      " |  \n",
      " |      NOTE\n",
      " |  \n",
      " |      The key ``'params'`` is used to store a list of parameter\n",
      " |      settings dicts for all the parameter candidates.\n",
      " |  \n",
      " |      The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and\n",
      " |      ``std_score_time`` are all in seconds.\n",
      " |  \n",
      " |      For multi-metric evaluation, the scores for all the scorers are\n",
      " |      available in the ``cv_results_`` dict at the keys ending with that\n",
      " |      scorer's name (``'_<scorer_name>'``) instead of ``'_score'`` shown\n",
      " |      above. ('split0_test_precision', 'mean_train_precision' etc.)\n",
      " |  \n",
      " |  best_estimator_ : estimator\n",
      " |      Estimator that was chosen by the search, i.e. estimator\n",
      " |      which gave highest score (or smallest loss if specified)\n",
      " |      on the left out data. Not available if ``refit=False``.\n",
      " |  \n",
      " |      See ``refit`` parameter for more information on allowed values.\n",
      " |  \n",
      " |  best_score_ : float\n",
      " |      Mean cross-validated score of the best_estimator\n",
      " |  \n",
      " |      For multi-metric evaluation, this is present only if ``refit`` is\n",
      " |      specified.\n",
      " |  \n",
      " |      This attribute is not available if ``refit`` is a function.\n",
      " |  \n",
      " |  best_params_ : dict\n",
      " |      Parameter setting that gave the best results on the hold out data.\n",
      " |  \n",
      " |      For multi-metric evaluation, this is present only if ``refit`` is\n",
      " |      specified.\n",
      " |  \n",
      " |  best_index_ : int\n",
      " |      The index (of the ``cv_results_`` arrays) which corresponds to the best\n",
      " |      candidate parameter setting.\n",
      " |  \n",
      " |      The dict at ``search.cv_results_['params'][search.best_index_]`` gives\n",
      " |      the parameter setting for the best model, that gives the highest\n",
      " |      mean score (``search.best_score_``).\n",
      " |  \n",
      " |      For multi-metric evaluation, this is present only if ``refit`` is\n",
      " |      specified.\n",
      " |  \n",
      " |  scorer_ : function or a dict\n",
      " |      Scorer function used on the held out data to choose the best\n",
      " |      parameters for the model.\n",
      " |  \n",
      " |      For multi-metric evaluation, this attribute holds the validated\n",
      " |      ``scoring`` dict which maps the scorer key to the scorer callable.\n",
      " |  \n",
      " |  n_splits_ : int\n",
      " |      The number of cross-validation splits (folds/iterations).\n",
      " |  \n",
      " |  refit_time_ : float\n",
      " |      Seconds used for refitting the best model on the whole dataset.\n",
      " |  \n",
      " |      This is present only if ``refit`` is not False.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The parameters selected are those that maximize the score of the left out\n",
      " |  data, unless an explicit score is passed in which case it is used instead.\n",
      " |  \n",
      " |  If `n_jobs` was set to a value higher than one, the data is copied for each\n",
      " |  point in the grid (and not `n_jobs` times). This is done for efficiency\n",
      " |  reasons if individual jobs take very little time, but may raise errors if\n",
      " |  the dataset is large and not enough memory is available.  A workaround in\n",
      " |  this case is to set `pre_dispatch`. Then, the memory is copied only\n",
      " |  `pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 *\n",
      " |  n_jobs`.\n",
      " |  \n",
      " |  See Also\n",
      " |  ---------\n",
      " |  :class:`ParameterGrid`:\n",
      " |      generates all the combinations of a hyperparameter grid.\n",
      " |  \n",
      " |  :func:`sklearn.model_selection.train_test_split`:\n",
      " |      utility function to split the data into a development set usable\n",
      " |      for fitting a GridSearchCV instance and an evaluation set for\n",
      " |      its final evaluation.\n",
      " |  \n",
      " |  :func:`sklearn.metrics.make_scorer`:\n",
      " |      Make a scorer from a performance metric or loss function.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      GridSearchCV\n",
      " |      BaseSearchCV\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, estimator, param_grid, *, scoring=None, n_jobs=None, iid='deprecated', refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs', error_score=nan, return_train_score=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseSearchCV:\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Call decision_function on the estimator with the best found parameters.\n",
      " |      \n",
      " |      Only available if ``refit=True`` and the underlying estimator supports\n",
      " |      ``decision_function``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  fit(self, X, y=None, *, groups=None, **fit_params)\n",
      " |      Run fit with all sets of parameters.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Training vector, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples, n_output)             or (n_samples,), default=None\n",
      " |          Target relative to X for classification or regression;\n",
      " |          None for unsupervised learning.\n",
      " |      \n",
      " |      groups : array-like of shape (n_samples,), default=None\n",
      " |          Group labels for the samples used while splitting the dataset into\n",
      " |          train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n",
      " |          instance (e.g., :class:`~sklearn.model_selection.GroupKFold`).\n",
      " |      \n",
      " |      **fit_params : dict of str -> object\n",
      " |          Parameters passed to the ``fit`` method of the estimator\n",
      " |  \n",
      " |  inverse_transform(self, Xt)\n",
      " |      Call inverse_transform on the estimator with the best found params.\n",
      " |      \n",
      " |      Only available if the underlying estimator implements\n",
      " |      ``inverse_transform`` and ``refit=True``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      Xt : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Call predict on the estimator with the best found parameters.\n",
      " |      \n",
      " |      Only available if ``refit=True`` and the underlying estimator supports\n",
      " |      ``predict``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Call predict_log_proba on the estimator with the best found parameters.\n",
      " |      \n",
      " |      Only available if ``refit=True`` and the underlying estimator supports\n",
      " |      ``predict_log_proba``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Call predict_proba on the estimator with the best found parameters.\n",
      " |      \n",
      " |      Only available if ``refit=True`` and the underlying estimator supports\n",
      " |      ``predict_proba``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  score(self, X, y=None)\n",
      " |      Returns the score on the given data, if the estimator has been refit.\n",
      " |      \n",
      " |      This uses the score defined by ``scoring`` where provided, and the\n",
      " |      ``best_estimator_.score`` method otherwise.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Input data, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples, n_output)             or (n_samples,), default=None\n",
      " |          Target relative to X for classification or regression;\n",
      " |          None for unsupervised learning.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Call transform on the estimator with the best found parameters.\n",
      " |      \n",
      " |      Only available if the underlying estimator supports ``transform`` and\n",
      " |      ``refit=True``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from BaseSearchCV:\n",
      " |  \n",
      " |  classes_\n",
      " |  \n",
      " |  n_features_in_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.MetaEstimatorMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(GridSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1950"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10*15*13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=DecisionTreeClassifier(), n_jobs=-1,\n",
       "             param_grid={'max_depth': array([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14]),\n",
       "                         'min_samples_leaf': array([ 1,  3,  5,  7,  9, 11, 13, 15, 17, 19]),\n",
       "                         'min_samples_split': array([10, 12, 14, 16, 18, 20, 22, 24, 26, 28])},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "param_grid = {'max_depth': np.arange(2,15),\n",
    "              'min_samples_split': np.arange(10,30,2),\n",
    "              'min_samples_leaf': np.arange(1,20,2)}\n",
    "gs = GridSearchCV(dt, param_grid, scoring='accuracy', n_jobs=-1, refit=True, cv=5)\n",
    "gs.fit(X_train_val,y_train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8182537191638943,\n",
       " {'max_depth': 9, 'min_samples_leaf': 5, 'min_samples_split': 10})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_score_, gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_submission(X_test, gs, './submission.csv', 'Survived')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
